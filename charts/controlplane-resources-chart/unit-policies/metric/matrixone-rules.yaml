labels:
  matrixone.cloud/rule-type: metric
  matrixone.cloud/unit-name: default
groups:
  - name: matrixone.rules
    rules:
      - alert: ServerStroageUsageAbsent
        expr: group by (matrixone_cloud_main_cluster) (up{matrixone_cloud_main_cluster!="", matrixone_cloud_component="cn"}) * 1 unless group by (matrixone_cloud_main_cluster) (server_storage_usage{matrixone_cloud_main_cluster!="", matrixone_cloud_component="cn"}) > 0
        for: 3m
        labels:
          severity: critical
          serviceScope: mo-cluster
          alertOwner: jacksonxie
          alertTeam: "Orch Team"
          mainCluster: |-
            {{ $labels.matrixone_cloud_main_cluster }}
        annotations:
          summary: server_storage_usage has absent for 3mins
          description: |-
            {{ $labels.matrixone_cloud_main_cluster }}: server_storage_usage has absent for 3mins
      - alert: ServerStroageUsageZero
        expr: avg by(account, matrixone_cloud_main_cluster) (server_storage_usage) < 0.01
        for: 30m
        labels:
          severity: warning
          serviceScope: mo-cluster
          alertOwner: jacksonxie
          alertTeam: "Orch Team"
          mainCluster: |-
            {{ $labels.matrixone_cloud_main_cluster }}
        annotations:
          summary: server_storage_usage below 0.01 MB for 30 min (more details on moc#2113)
          description: |-
            server_storage_usage below 0.01 MB for 30 min, cluster {{ $labels.matrixone_cloud_main_cluster }}, account {{ $labels.account }}
      - alert: MOPodCrashLoop
        # FIXME: remove the name regex hack, we may join other metrics to determine MO Pod accurately
        expr: sum(increase(kube_pod_container_status_restarts_total{job="kube-state-metrics", pod=~".*(-dn-|-cn-|-log-|proxy-proxy-).*"}[30m])) by (namespace, pod) > 2
        for: 2m
        labels:
          severity: critical
          serviceScope: mo-cluster
          alertOwner: aylei
          alertTeam: "Orch Team"
        annotations:
          summary: MO Pod crash loop in last 30 minutes
          description: |-
            {{ $labels.namespace }}/{{ $labels.pod }}: MO Pod restarted {{ $value }} times for last 30 minutes
      - alert: mo_oom
        expr: (sum(increase(kube_pod_container_status_restarts_total[5m])) by (namespace, pod) > 0) + on (namespace, pod) (sum(kube_pod_container_status_last_terminated_reason{reason="OOMKilled", pod=~".*-(cn|dn|log|proxy)-.*"}) by (namespace, pod))
        for: 0m
        labels:
          alertname: MO_OOMKilled
          severity: "critical"
          serviceScope: "mo-cluster"
          alertOwner: "aylei"
          alertTeam: "Orch Team"
        annotations:
          summary: "{{$labels.namespace}}.{{$labels.pod}} mo get OOMKilled"
          description: "{{$labels.namespace}}.{{$labels.pod}} mo get OOMKilled"
      - alert: cn_avg_cpu_high
        expr: sum(kube_horizontalpodautoscaler_status_target_metric{metric_name="cpu", horizontalpodautoscaler!="keda-hpa-default-localpv"}) by (namespace, horizontalpodautoscaler) > 70
        for: 5m
        labels:
          alertname: CN_Avg_CPU_High
          severity: "warning"
          serviceScope: "mo-cluster"
          alertOwner: "aylei"
          alertTeam: "Orch Team"
        annotations:
          summary: "{{$labels.namespace}}.{{$labels.horizontalpodautoscaler}} CPU high"
          description: "{{$labels.namespace}}.{{$labels.horizontalpodautoscaler}} CPU avg get over 70%, may need manual scaling"
      - alert: cn_may_need_scalein
        expr: sum(kube_horizontalpodautoscaler_status_target_metric{metric_name="cpu", horizontalpodautoscaler!~".*-default-localpv"}) by (namespace, horizontalpodautoscaler) < 24 and sum(kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler!~".*-default-localpv"}) by (namespace, horizontalpodautoscaler) > 1
        for: 5m
        labels:
          alertname: CN_MAY_NEED_SCALE_IN
          severity: "warning"
          serviceScope: "mo-cluster"
          alertOwner: "aylei"
          alertTeam: "Orch Team"
        annotations:
          summary: "{{$labels.namespace}}.{{$labels.horizontalpodautoscaler}} CPU low and did not scale-in"
          description: "{{$labels.namespace}}.{{$labels.horizontalpodautoscaler}} CPU avg is below 24%, but did not scale-in, may need manual operation"
  - name: infra.rules
    rules:
      - alert: PodCrashLoop
        expr: sum(increase(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[30m])) by (namespace, pod) > 3
        for: 2m
        labels:
          severity: "warning"
          serviceScope: mo-cloud
          alertOwner: aylei
          alertTeam: "Orch Team"
        annotations:
          summary: Pod crash loop in last 30 minutes
          description: |-
            {{ $labels.namespace }}/{{ $labels.pod }}: Pod restarted {{ $value }} times for last 30 minutes
      - alert: DiskIOUtil
        expr: sum(rate(node_disk_io_time_seconds_total{job="kubernetes-service-endpoints", device=~"vda"}[2m])) by (node, device) > 0.8
        for: 5m
        labels:
          severity: "warning"
          serviceScope: mo-cloud
          alertOwner: aylei
          alertTeam: "Orch Team"
        annotations:
          summary: Disk IO util higher than 80% for 5 minutes
          description: |-
            {{ $labels.node }} Node disk {{ $labels.device }} IO util {{ $value }} higher than 80% for 5 minutes
      - alert: CNDrainingTooLong
        expr: sum(up{lifecycle_apps_kruise_io_state=~"PreparingUpdate|PreparingDelete"}) by (namespace, pod) > 0
        for: 10m
        labels:
          severity: "warning"
          serviceScope: mo-cloud
          alertOwner: aylei
          alertTeam: "Orch Team"
        annotations:
          summary: CN stuck in draining state for more than 10 minutes
          description: |-
            {{ $labels.namespace }}/{{ $labels.pod }} stuck in draining state for more than 10 minutes
