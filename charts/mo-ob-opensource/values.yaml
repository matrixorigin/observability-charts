alerting:
  namespace: "mo-ob"

# swith for generate alertrules which defined in `rules` folder
# - cooperate with template templates/mo-alerting-rules.yaml
# swith for generate alertrules which defined in `alert-rules/log-alert-rules` folder
# - cooperate with template templates/log-alert-rules.yaml
alertrules:
  enabled: false

defaultDatasource:
  # from matrixorigin/ops
  # cooperate with template templates/loki-datasource.yaml
  loki: true

promtail:
  enabled: true
  serviceMonitor:
    enabled: true
  tolerations: 
  - operator: Exists
  extraArgs:
    - -config.expand-env=true
  config:
    logLevel: info
    logFormat: logfmt
    clients:
      - url: http://loki-gateway/loki/api/v1/push
        batchwait: 1s
        batchsize: 1048576
    snippets:
      # -- You can put here any keys that will be directly added to the config file's 'limits_config' block.
      # Limits Config related issue: https://github.com/matrixorigin/MO-Cloud/issues/1292
      extraLimitsConfig: |
        max_line_size: 256kb
        max_line_size_truncate: true
      extraRelabelConfigs:
        # keep all kubernetes pod's labels
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
      extraScrapeConfigs: |
        - job_name: host_log
          static_configs:
          - targets:
              - localhost
            labels:
              job: host_messages
              __path__: /var/log/host/messages
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_boot_log
              __path__: /var/log/host/boot.log
              node_name: '${HOSTNAME}'
          - targets:
            - localhost
            labels:
              job: host_btmp
              __path__: /var/log/host/btmp
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_cron
              __path__: /var/log/host/cron
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_maillog
              __path__: /var/log/host/maillog
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_secure
              __path__: /var/log/host/secure
              node_name: '${HOSTNAME}'
  # Extra volumes to be added in addition to those specified under `defaultVolumes`.
  extraVolumes:
  - name: local
    hostPath:
      # path: /var/log/messages
      path: /var/log

  # Extra volume mounts together. Corresponds to `extraVolumes`.
  extraVolumeMounts:
  - name: local
    mountPath: /var/log/host
    readOnly: true

loki:
  enabled: true
  tableManager:
    enabled: true
    # retention related issue: https://github.com/matrixone-cloud/observability/issues/86
    retention_deletes_enabled: true
    retention_period: 720h
  loki:
    # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
    # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/loki/latest/configuration/#use-environment-variables-in-the-configuration).
    # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
    configStorageType: ConfigMap
    # Should authentication be enabled
    auth_enabled: false
    # -- Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.
    server:
    # these timeout should smaller than grafana's dataproxy timeout
      http_server_read_timeout: 300s
      http_server_write_timeout: 300s
    # -- Limits config
    limits_config:
      #enforce_metric_name: false ### removed in loki 3.0.0
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_cache_freshness_per_query: 10m
      split_queries_by_interval: 15m
      max_label_names_per_series: 30
    query_scheduler:
      max_outstanding_requests_per_tenant: 10000
    frontend:
      max_outstanding_per_tenant: 10000
    # -- Storage config. Providing this will automatically populate all necessary storage configs in the templated config.
    storage:
      bucketNames:
        # use same bucket
        chunks: test-bucket
        ruler: test-bucket
      type: s3
      s3:
        # s3: null
        # e.g. s3.us-west-2.amazonaws.com
        endpoint: null
        # e.g. us-west-2
        region: null
        secretAccessKey: null
        accessKeyId: null
        s3ForcePathStyle: false
        insecure: false
    # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
    schemaConfig: 
      configs:
      - from: 2024-04-01
        object_store: s3
        store: tsdb
        schema: v13
        index:
          prefix: index_
          period: 24h
    # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler
    rulerConfig: 
      evaluation_interval: 1m
      poll_interval: 1m
      storage: 
        type: local
        local: 
          directory: /rules
      alertmanager_url: http://mo-ob-alertmanager.mo-ob:9093
    # -- Additional query scheduler config
    ### use default config, old config not work in loki 3.0.0
    #storage_config:
    #  boltdb_shipper:
    #    active_index_directory: /var/loki/data/loki/boltdb-shipper-active
    #    cache_location: /var/loki/data/loki/boltdb-shipper-cache
    #    cache_ttl: 24h         # Can be increased for faster performance over longer query periods, uses more disk space
    #    shared_store: s3
    #  tsdb_shipper:
    #    active_index_directory: /var/loki/data/loki/tsdb-index
    #    cache_location: /var/loki/data/loki/tsdb-cache
    #    shared_store: s3
    #  # alibabacloud:
    #  #   bucket: <bucket>
    #  #   endpoint: <endpoint>
    #  #   access_key_id: <access_key_id>
    #  #   secret_access_key: <secret_access_key>
  sidecar:
    rules:
      label: loki_rule
      labelValue: "true"
      folder: /rules/fake
      searchNamespace: ALL
  write:
    # -- Number of replicas for the write
    replicas: 3
    persistence:
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
    resources: 
      requests:
        memory: "4Gi"
        cpu: "250m"
      limits:
        memory: "4Gi"
        cpu: "2000m"

  read:
    replicas: 1
    persistence:
      enableStatefulSetAutoDeletePVC: true
      size: 10Gi
      storageClass: null
      selector: null
    resources: 
      requests:
        memory: "2Gi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

  backend:
    replicas: 1
    persistence:
      enableStatefulSetAutoDeletePVC: true
      size: 10Gi
      storageClass: null
      selector: null
    resources: 
      requests:
        memory: "2Gi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

  # Configuration for the gateway
  gateway:
    enabled: true
    replicas: 1
    resources: 
      requests:
        memory: "250Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"


  test:
    enabled: false

  # Monitoring section determines which monitoring features to enable
  monitoring:
    # Dashboards for monitoring Loki
    dashboards:
      # -- If enabled, create configmap with dashboards for monitoring Loki
      enabled: true
      labels:
        grafana_dashboard: "1"
    # Recording rules for monitoring Loki, required for some dashboards
    rules:
      # -- If enabled, create PrometheusRule resource with Loki recording rules
      enabled: false
      # -- Include alerting rules
      alerting: false
    # ServiceMonitor configuration
    serviceMonitor:
      # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
      enabled: true

    # Self monitoring determines whether Loki should scrape its own logs.
    # This feature currently relies on the Grafana Agent Operator being installed,
    # which is installed by default using the grafana-agent-operator sub-chart.
    # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure
    # scrape configs to scrape its own logs with the labels expected by the included dashboards.
    selfMonitoring:
      enabled: false
      # -- Tenant to use for self monitoring

      # Grafana Agent configuration
      grafanaAgent:
        # -- Controls whether to install the Grafana Agent Operator and its CRDs.
        # Note that helm will not install CRDs if this flag is enabled during an upgrade.
        # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds
        installOperator: false

    # The Loki canary pushes logs to and queries from this loki installation to test
    # that it's working correctly
    lokiCanary:
      enabled: false

kube-prometheus-stack:
  enabled: true
  alertmanager:
    enabled: false
  grafana:
    enabled: false
    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    forceDeployDatasources: true
  nodeExporter:
    enabled: true
  prometheus-node-exporter:
    prometheus:
      monitor:
        enabled: false
    tolerations:
    - operator: Exists
  kubeStateMetrics:
    enabled: true
  kube-state-metrics:
    enabled: true
    prometheus:
      monitor:
        enabled: false

  prometheusOperator:
    enabled: true
    # image:
    #   registry: registry.cn-hangzhou.aliyuncs.com
    #   repository: prometheus-operator
    #   tag: ""
    #   pullPolicy: Always
    tls:
      enabled: false
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false

    # prometheusConfigReloader:
    #   image:
    #     registry: registry.cn-hangzhou.aliyuncs.com/ack-cos
    #     repository: prometheus-config-reloader
    #     tag: ""

  prometheus:
    prometheusSpec:
      externalLabels:
        # cluster provider
        clusterDetail: aliyun-new-dev-controlplane
      logLevel: debug
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      # image:
      #   registry: registry.cn-hangzhou.aliyuncs.com
      #   repository: prometheus
      #   tag: v2.42.0
      #   sha: ""
      resources:
        limits:
          cpu: 2000m
          memory: "4Gi"
        requests:
          cpu: 2000m
          memory: "4Gi"
      # [必要]根据部署的云厂商选择存储配置
      storageSpec:
        ## Using PersistentVolumeClaim
        ##
        volumeClaimTemplate:
          spec:
            # aws使用gp3存储
            # storageClassName: "gp3"
            # alicloud使用以下sc
            storageClassName: "alicloud-disk-essd"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 40Gi
      additionalScrapeConfigs: 
        # 过渡方案：在相关组件添加完成 annotation 后会被删除
        - job_name: "mocloud-application"
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: "mo-cloud|cos"
              source_labels:
              - __meta_kubernetes_namespace
            - action: drop
              source_labels: [__meta_kubernetes_pod_name, __meta_kubernetes_pod]
              regex: frps
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod
        # Scrape config for service endpoints.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape services that have a value of
        # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints'
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: drop
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

        # Scrape config for slow service endpoints; same as above, but with a larger
        # timeout and a larger interval
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints-slow'
          honor_labels: true

          scrape_interval: 5m
          scrape_timeout: 30s

          kubernetes_sd_configs:
            - role: endpoints

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

        - job_name: "moob-application"
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: "mo-ob"
              source_labels:
              - __meta_kubernetes_namespace
            - source_labels:
              - __meta_kubernetes_endpoints_label_need_metrics
              action: keep
              regex: true
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_endpoints_label_(.+)
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
              - __meta_kubernetes_service_name
              target_label: service
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: node
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
        - job_name: "matrixone-cn-pods"
          honor_labels: true
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_labelpresent_matrixorigin_io_component]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: "true"
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

      alertingEndpoints:
        - name: "mo-ob-alertmanager"
          # 如果跨ns，需要修改
          namespace: "mo-ob"
          port: 9093
          scheme: http
          pathPrefix: ""
          apiVersion: v2

  thanosRuler:
    enabled: false
