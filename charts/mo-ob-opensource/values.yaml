alerting:
  namespace: "mo-ob"

# swith for generate alertrules which defined in `rules` folder
# - cooperate with template templates/mo-alerting-rules.yaml
# swith for generate alertrules which defined in `alert-rules/log-alert-rules` folder
# - cooperate with template templates/log-alert-rules.yaml
alertrules:
  enabled: false

## handle chart kube-prometheus-stack's sub-chart serviceAccount create with imagePullSecrets case
## cc https://github.com/matrixorigin/MO-Cloud/issues/4464
## ---- IMPORTANT ----
## this section MUST eq the 'kube-prometheus-stack' section, like:
# kube-prometheus-stack:
#   kube-state-metrics:
#     enabled: true
#     serviceAccount:
#       create: true
#       name:
serviceAccount:
  kubeStateMetrics:
    handle: false
    ## if handle = ture, pls sync this value with kube-state-metrics.serviceAccount.name
    name: "mo-ob-opensource-kube-state-metrics"

defaultDatasource:
  # from matrixorigin/ops
  # cooperate with template templates/loki-datasource.yaml
  loki: true

## common
nameOverride: ""
namespaceOverride: ""
selectorOverride: {}
customLabels: []
releaseLabel: []

# alloy for k8s events
alloy:
  image:
    registry: "docker.io"
    repository: grafana/alloy
    tag: v1.3.1
  configReloader:
    enabled: true
    image:
      registry: "ghcr.io"
      repository: jimmidyson/configmap-reload
      tag: v0.12.0
  enabled: true
  alloy:
    resources: 
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 200m
        memory: 200Mi
    configMap:
      # -- Content to assign to the new ConfigMap.  This is passed into `tpl` allowing for templating from values.
      content: |
        loki.source.kubernetes_events "k8s_event" {
          forward_to = [loki.write.local.receiver]
        }

        loki.write "local" {
          endpoint {
            url = "http://{{ .Release.Name }}-loki-gateway/loki/api/v1/push"
          }
        }

promtail:
  enabled: true
  serviceMonitor:
    enabled: true
  tolerations: 
  - operator: Exists
  extraArgs:
    - -config.expand-env=true
  config:
    logLevel: info
    logFormat: logfmt
    clients:
      - url: http://{{ .Release.Name }}-loki-gateway/loki/api/v1/push
        batchwait: 1s
        batchsize: 1048576
    snippets:
      # -- You can put here any keys that will be directly added to the config file's 'limits_config' block.
      # Limits Config related issue: https://github.com/matrixorigin/MO-Cloud/issues/1292
      extraLimitsConfig: |
        max_line_size: 256kb
        max_line_size_truncate: true
      extraRelabelConfigs:
        # keep all kubernetes pod's labels
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
      extraScrapeConfigs: |
        - job_name: host_log
          static_configs:
          - targets:
              - localhost
            labels:
              job: host_messages
              __path__: /var/log/host/messages
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_boot_log
              __path__: /var/log/host/boot.log
              node_name: '${HOSTNAME}'
          - targets:
            - localhost
            labels:
              job: host_btmp
              __path__: /var/log/host/btmp
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_cron
              __path__: /var/log/host/cron
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_maillog
              __path__: /var/log/host/maillog
              node_name: '${HOSTNAME}'
          - targets:
              - localhost
            labels:
              job: host_secure
              __path__: /var/log/host/secure
              node_name: '${HOSTNAME}'
  # Extra volumes to be added in addition to those specified under `defaultVolumes`.
  extraVolumes:
  - name: local
    hostPath:
      # path: /var/log/messages
      path: /var/log

  # Extra volume mounts together. Corresponds to `extraVolumes`.
  extraVolumeMounts:
  - name: local
    mountPath: /var/log/host
    readOnly: true

loki:
  enabled: true
  tableManager:
    enabled: true
    # retention related issue: https://github.com/matrixone-cloud/observability/issues/86
    retention_deletes_enabled: true
    retention_period: 720h
  lokiCanary:
    enabled: false
  
  chunksCache:
    enabled: false
  
  resultsCache:
    enabled: false
  
  loki:
    # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
    # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/loki/latest/configuration/#use-environment-variables-in-the-configuration).
    # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
    configStorageType: ConfigMap
    # Should authentication be enabled
    auth_enabled: false
    # -- Check https://grafana.com/docs/loki/latest/configuration/#server for more info on the server configuration.
    server:
    # these timeout should smaller than grafana's dataproxy timeout
      http_server_read_timeout: 300s
      http_server_write_timeout: 300s
    # -- Limits config
    limits_config:
      #enforce_metric_name: false ### removed in loki 3.0.0
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_cache_freshness_per_query: 10m
      split_queries_by_interval: 15m
      max_label_names_per_series: 30
    query_scheduler:
      max_outstanding_requests_per_tenant: 10000
    frontend:
      max_outstanding_per_tenant: 10000
    # -- Storage config. Providing this will automatically populate all necessary storage configs in the templated config.
    storage:
      bucketNames:
        chunks: test-bucket
      type: s3
      s3:
        endpoint: minio.loki-tenant
        accessKeyId: obtest-access
        secretAccessKey: obtest-secret
        s3ForcePathStyle: true
        insecure: true
    # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
    schemaConfig: 
      configs:
      - from: 2024-04-01
        object_store: s3
        store: tsdb
        schema: v13
        index:
          prefix: index_
          period: 24h
    # -- Check https://grafana.com/docs/loki/latest/configuration/#ruler for more info on configuring ruler
    rulerConfig: 
      evaluation_interval: 1m
      poll_interval: 1m
      storage: 
        type: local
        local: 
          directory: /rules
      # alertmanager 单节点
      alertmanager_url: http://mo-ob-alertmanager.mo-ob:9093
      # alertmanager 3节点集群使用：
      # alertmanager_url: http://mo-ob-alertmanager-0.mo-ob:9093,http://mo-ob-alertmanager-1.mo-ob:9093,http://mo-ob-alertmanager-2.mo-ob:9093 
      enable_alertmanager_v2: true
      enable_api: true
      alertmanager_client:
        type: "Basic"
        credentials_file: "/tmp/loki/alertmanager-loki-credentials"

    
    # -- Additional query scheduler config
    ### use default config, old config not work in loki 3.0.0
    storage_config:
      # bucket subpath
      object_prefix: ""
    #  boltdb_shipper:
    #    active_index_directory: /var/loki/data/loki/boltdb-shipper-active
    #    cache_location: /var/loki/data/loki/boltdb-shipper-cache
    #    cache_ttl: 24h         # Can be increased for faster performance over longer query periods, uses more disk space
    #    shared_store: s3
    #  tsdb_shipper:
    #    active_index_directory: /var/loki/data/loki/tsdb-index
    #    cache_location: /var/loki/data/loki/tsdb-cache
    #    shared_store: s3
    #  # alibabacloud:
    #  #   bucket: <bucket>
    #  #   endpoint: <endpoint>
    #  #   access_key_id: <access_key_id>
    #  #   secret_access_key: <secret_access_key>
  sidecar:
    rules:
      label: loki_rule
      labelValue: "true"
      folder: /rules/fake
      searchNamespace: ALL
  write:
    # -- Number of replicas for the write
    replicas: 3
    persistence:
      # -- Enable StatefulSetAutoDeletePVC feature
      enableStatefulSetAutoDeletePVC: false
      # -- Size of persistent disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Selector for persistent disk
      selector: null
    resources: 
      requests:
        memory: "4Gi"
        cpu: "250m"
      limits:
        memory: "4Gi"
        cpu: "2000m"

  read:
    replicas: 1
    persistence:
      enableStatefulSetAutoDeletePVC: true
      size: 10Gi
      storageClass: null
      selector: null
    resources: 
      requests:
        memory: "2Gi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"

  backend:
    replicas: 1
    persistence:
      enableStatefulSetAutoDeletePVC: true
      size: 10Gi
      storageClass: null
      selector: null
    resources: 
      requests:
        memory: "2Gi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"
    # -- Volume mounts to add to the backend pods
    extraVolumeMounts:
    - name: alertmanager-credentials
      mountPath: /tmp/loki
      readOnly: true
    extraVolumes: 
    - name: alertmanager-credentials
      secret:
        secretName: alertmanager-loki-credentials

  # Configuration for the gateway
  gateway:
    enabled: true
    replicas: 1
    resources: 
      requests:
        memory: "250Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "2000m"


  test:
    enabled: false

  # Monitoring section determines which monitoring features to enable
  monitoring:
    # Dashboards for monitoring Loki
    dashboards:
      # -- If enabled, create configmap with dashboards for monitoring Loki
      enabled: true
      labels:
        grafana_dashboard: "1"
    # Recording rules for monitoring Loki, required for some dashboards
    rules:
      # -- If enabled, create PrometheusRule resource with Loki recording rules
      enabled: false
      # -- Include alerting rules
      alerting: false
    # ServiceMonitor configuration
    serviceMonitor:
      # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
      enabled: true

    # Self monitoring determines whether Loki should scrape its own logs.
    # This feature currently relies on the Grafana Agent Operator being installed,
    # which is installed by default using the grafana-agent-operator sub-chart.
    # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure
    # scrape configs to scrape its own logs with the labels expected by the included dashboards.
    selfMonitoring:
      enabled: false
      # -- Tenant to use for self monitoring

      # Grafana Agent configuration
      grafanaAgent:
        # -- Controls whether to install the Grafana Agent Operator and its CRDs.
        # Note that helm will not install CRDs if this flag is enabled during an upgrade.
        # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds
        installOperator: false

    # The Loki canary pushes logs to and queries from this loki installation to test
    # that it's working correctly
    lokiCanary:
      enabled: false


vm:
  enabled: true

  # -- VictoriaMetrics Operator dependency chart configuration. More values can be found [here](https://docs.victoriametrics.com/helm/victoriametrics-operator#parameters). Also checkout [here](https://docs.victoriametrics.com/operator/vars) possible ENV variables to configure operator behaviour
  victoria-metrics-operator:
    enabled: true
    crds:
      plain: true
      cleanup:
        enabled: true
        image:
          repository: bitnami/kubectl
          pullPolicy: IfNotPresent
    serviceMonitor:
      enabled: true
    operator:
      # -- By default, operator converts prometheus-operator objects.
      disable_prometheus_converter: false

  defaultDashboards:
    # -- Enable custom dashboards installation
    enabled: true
    defaultTimezone: utc
    labels: {}
    annotations: {}
    grafanaOperator:
      # -- Create dashboards as CRDs (requires grafana-operator to be installed)
      enabled: false
      spec:
        instanceSelector:
          matchLabels:
            dashboards: grafana
        allowCrossNamespaceImport: false
    # -- Create dashboards as ConfigMap despite dependency it requires is not installed
    dashboards:
      victoriametrics-vmalert:
        enabled: true
      victoriametrics-operator:
        enabled: true
      # -- In ArgoCD using client-side apply this dashboard reaches annotations size limit and causes k8s issues without server side apply
      # See [this issue](https://github.com/VictoriaMetrics/helm-charts/tree/disable-node-exporter-dashboard-by-default/charts/victoria-metrics-k8s-stack#metadataannotations-too-long-must-have-at-most-262144-bytes-on-dashboards)
      node-exporter-full:
        enabled: false

  # -- Create default rules for monitoring the cluster
  defaultRules:
    create: false

  # Configures vmsingle params
  vmsingle:
    # -- VMSingle annotations
    annotations: {}
    # -- Create VMSingle CR
    enabled: true
    # -- Full spec for VMSingle CRD. Allowed values describe [here](https://docs.victoriametrics.com/operator/api#vmsinglespec)
    spec:
      port: "8429"
      # -- Data retention period. Possible units character: h(ours), d(ays), w(eeks), y(ears), if no unit character specified - month. The minimum retention period is 24h. See these [docs](https://docs.victoriametrics.com/single-server-victoriametrics/#retention)
      retentionPeriod: "5d"
      replicaCount: 1
      storage:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 40Gi      


  alertmanager:
    # -- Create VMAlertmanager CR
    enabled: false

  vmalert:
    # -- VMAlert annotations
    annotations: {}
    # -- Create VMAlert CR
    enabled: true

    # -- Controls whether VMAlert should use VMAgent or VMInsert as a target for remotewrite
    remoteWriteVMAgent: false
    # -- (object) Full spec for VMAlert CRD. Allowed values described [here](https://docs.victoriametrics.com/operator/api#vmalertspec)
    spec:
      port: "8080"
      selectAllByDefault: true
      evaluationInterval: 15s
      extraArgs:
        http.pathPrefix: "/"

      # External labels to add to all generated recording rules and alerts
      externalLabels: {}

    # -- (object) Extra VMAlert annotation templates
    templateFiles:
      {}
      # template_1.tmpl: |-
      #   {{ define "hello" -}}
      #   hello, Victoria!
      #   {{- end }}
      # template_2.tmpl: ""

    # -- Allows to configure static notifiers, discover notifiers via Consul and DNS,
    # see specification [here](https://docs.victoriametrics.com/vmalert/#notifier-configuration-file).
    # This configuration will be created as separate secret and mounted to VMAlert pod.
    additionalNotifierConfigs: {}
      # dns_sd_configs:
      #   - names:
      #       - my.domain.com
      #     type: 'A'
      #     port: 9093

  vmagent:
    # -- Create VMAgent CR
    enabled: true
    # -- VMAgent annotations
    annotations: {}
    # -- (object) Full spec for VMAgent CRD. Allowed values described [here](https://docs.victoriametrics.com/operator/api#vmagentspec)
    spec:
      port: "8429"
      selectAllByDefault: true
      scrapeInterval: 20s
      externalLabels: {}
        # For multi-cluster setups it is useful to use "cluster" label to identify the metrics source.
        # For example:
        # cluster: cluster-name
      extraArgs:
        promscrape.streamParse: "true"
        # Do not store original labels in vmagent's memory by default. This reduces the amount of memory used by vmagent
        # but makes vmagent debugging UI less informative. See: https://docs.victoriametrics.com/vmagent/#relabel-debug
        promscrape.dropOriginalLabels: "true"

      hosts:
        - vmagent.domain.com
      extraPaths: []
      # - path: /*
      #   pathType: Prefix
      #   backend:
      #     service:
      #       name: ssl-redirect
      #       port:
      #         name: service


  defaultDatasources:
    victoriametrics:
      # -- Create per replica prometheus compatible datasource
      perReplica: false
      # -- List of prometheus compatible datasource configurations.
      # VM `url` will be added to each of them in templates.
      datasources:
        - name: VictoriaMetrics
          type: prometheus
          isDefault: true
        - name: VictoriaMetrics (DS)
          isDefault: false
          type: victoriametrics-datasource

  # -- Grafana dependency chart configuration. For possible values refer [here](https://github.com/grafana/helm-charts/tree/main/charts/grafana#configuration)
  grafana:
    enabled: false

  # -- prometheus-node-exporter dependency chart configuration. For possible values check [here](https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-node-exporter/values.yaml)
  prometheus-node-exporter:
    enabled: false

  # -- kube-state-metrics dependency chart configuration. For possible values check [here](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml)
  kube-state-metrics:
    enabled: false

  # -- Component scraping the kubelets
  kubelet:
    enabled: false

  # Component scraping the kube api server
  kubeApiServer:
    # -- Enable Kube Api Server metrics scraping
    enabled: false

  # Component scraping the kube controller manager
  kubeControllerManager:
    # -- Enable kube controller manager metrics scraping
    enabled: false

  # Component scraping kubeDns. Use either this or coreDns
  kubeDns:
    # -- Enabled KubeDNS metrics scraping
    enabled: false

  # Component scraping coreDns. Use either this or kubeDns
  coreDns:
    # -- Enabled CoreDNS metrics scraping
    enabled: false

  # Component scraping etcd
  kubeEtcd:
    # -- Enabled KubeETCD metrics scraping
    enabled: false

  # Component scraping kube scheduler
  kubeScheduler:
    # -- Enable KubeScheduler metrics scraping
    enabled: false

  # Component scraping kube proxy
  kubeProxy:
    # -- Enable kube proxy metrics scraping
    enabled: false

kube-prometheus-stack:
  enabled: true
  alertmanager:
    enabled: false
  grafana:
    enabled: false
    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    forceDeployDatasources: true
  nodeExporter:
    enabled: true
  prometheus-node-exporter:
    prometheus:
      monitor:
        enabled: false
    tolerations:
    - operator: Exists
  kubeStateMetrics:
    enabled: true
  kube-state-metrics:
    enabled: true
    prometheus:
      monitor:
        enabled: false
    serviceAccount:
      create: true
      name:

  prometheusOperator:
    enabled: true
    # image:
    #   registry: registry.cn-hangzhou.aliyuncs.com
    #   repository: prometheus-operator
    #   tag: ""
    #   pullPolicy: Always
    tls:
      enabled: false
    admissionWebhooks:
      enabled: false
      patch:
        enabled: false

    # prometheusConfigReloader:
    #   image:
    #     registry: registry.cn-hangzhou.aliyuncs.com/ack-cos
    #     repository: prometheus-config-reloader
    #     tag: ""

  prometheus:
    prometheusSpec:
      externalLabels:
        # cluster provider
        clusterDetail: aliyun-new-dev-controlplane
      logLevel: debug
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      # image:
      #   registry: registry.cn-hangzhou.aliyuncs.com
      #   repository: prometheus
      #   tag: v2.42.0
      #   sha: ""
      resources:
        limits:
          cpu: 2000m
          memory: "4Gi"
        requests:
          cpu: 2000m
          memory: "4Gi"
      # [必要]根据部署的云厂商选择存储配置
      storageSpec:
        ## Using PersistentVolumeClaim
        ##
        volumeClaimTemplate:
          spec:
            # aws使用gp3存储
            # storageClassName: "gp3"
            # alicloud使用以下sc
            storageClassName: "alicloud-disk-essd"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 40Gi
      additionalScrapeConfigs: 
        # Scrape config for service endpoints.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape services that have a value of
        # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints'
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: drop
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

        # Scrape config for slow service endpoints; same as above, but with a larger
        # timeout and a larger interval
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
        # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
        # to set this to `https` & most likely set the `tls_config` of the scrape config.
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: If the metrics are exposed on a different port to the
        # service then set this appropriately.
        # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
        # then you can set any parameter
        - job_name: 'kubernetes-service-endpoints-slow'
          honor_labels: true

          scrape_interval: 5m
          scrape_timeout: 30s

          kubernetes_sd_configs:
            - role: endpoints

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              action: replace
              target_label: __scheme__
              regex: (https?)
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
              replacement: __param_$1
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              action: replace
              target_label: service
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

        - job_name: "moob-application"
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
          relabel_configs:
            - action: keep
              regex: "mo-ob"
              source_labels:
              - __meta_kubernetes_namespace
            - source_labels:
              - __meta_kubernetes_endpoints_label_need_metrics
              action: keep
              regex: true
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: labelmap
              regex: __meta_kubernetes_endpoints_label_(.+)
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
              - __meta_kubernetes_service_name
              target_label: service
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: node
            - action: replace
              source_labels: [__meta_kubernetes_pod_name]
              target_label: pod
        - job_name: "matrixone-cn-pods"
          honor_labels: true
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_labelpresent_matrixorigin_io_component]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: "true"
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              target_label: __address__
              regex: (.+?)(?::\d+)?;(\d+)
              replacement: $1:$2
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: node
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: pod

      alertingEndpoints:
      # 单节点部署
        - name: "mo-ob-alertmanager"
          # 如果跨ns，需要修改
          namespace: "mo-ob"
          port: 9093
          scheme: http
          pathPrefix: ""
          apiVersion: v2
          basicAuth:
            username: 
              key: username
              name: alertmanager-auth-secret
            password: 
              key: password
              name: alertmanager-auth-secret
      #集群部署
        # - name: "mo-ob-alertmanager-0"
        #   # 如果跨ns，需要修改
        #   namespace: "mo-ob"
        #   port: 9093
        #   scheme: http
        #   pathPrefix: ""
        #   apiVersion: v2
        #   basicAuth:
        #     username: 
        #       key: username
        #       name: alertmanager-auth-secret
        #     password: 
        #       key: password
        #       name: alertmanager-auth-secret
        # - name: "mo-ob-alertmanager-1"
        #   # 如果跨ns，需要修改
        #   namespace: "mo-ob"
        #   port: 9093
        #   scheme: http
        #   pathPrefix: ""
        #   apiVersion: v2
        #   basicAuth:
        #     username: 
        #       key: username
        #       name: alertmanager-auth-secret
        #     password: 
        #       key: password
        #       name: alertmanager-auth-secret
        # - name: "mo-ob-alertmanager-2"
        #   # 如果跨ns，需要修改
        #   namespace: "mo-ob"
        #   port: 9093
        #   scheme: http
        #   pathPrefix: ""
        #   apiVersion: v2
        #   basicAuth:
        #     username: 
        #       key: username
        #       name: alertmanager-auth-secret
        #     password: 
        #       key: password
        #       name: alertmanager-auth-secret


  thanosRuler:
    enabled: false
